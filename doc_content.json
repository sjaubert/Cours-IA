[
  "The 7 Secret Knobs That Control Every AI Response",
  "Temperature, Top-P, and Max Tokens â€” the hidden parameters behind every ChatGPT answer. Learn how to master them all.",
  "Every time you hit â€œsendâ€ to ChatGPT, Claude, or any LLM, seven invisible parameters are silently shaping the response. Change one number, and you go from genius insights to nonsensical rambling.",
  "Most people never touch these settings. They stick with defaults and wonder why AI sometimes feels â€œdumb.â€ Master these 7 parameters, and youâ€™ll get better outputs than 99% of users.",
  "Non-members? Read this story free with this link ğŸ‘‰Â Non-members link",
  "Press enter or click to view image in full size",
  "Table of Contents",
  "Â·Â The Probability Machine Youâ€™re Actually Using\nÂ·Â Parameter 1: Max Tokens (The Length Controller)\nÂ·Â Parameter 2: Temperature (The Creativity Dial)\nÂ·Â Parameter 3: Top-P / Nucleus Sampling (The Quality Filter)\nÂ·Â Parameter 4: Top-K (The Candidate Limiter)\nÂ·Â Parameter 5: Frequency Penalty (The Repetition Killer)\nÂ·Â Parameter 6: Presence Penalty (The Novelty Driver)\nÂ·Â Parameter 7: Stop Sequences (The Emergency Brake)\nÂ·Â How Parameters Interact (The Critical Part)\nÂ·Â Real Scenarios: What Settings to Use When\nÂ·Â The Cheat Sheet Youâ€™ll Actually Use\nÂ·Â Your Next Steps",
  "The Probability Machine Youâ€™re Actually Using",
  "Hereâ€™s what really happens when you ask AI a question:",
  "You type:Â â€œWrite a product descriptionâ€",
  "What actually happens:",
  "Your text becomes tokens (numbers)",
  "Model calculates probability for every possible next token",
  "Seven parameters decide which token gets chosen",
  "Repeat thousands of times until done",
  "Youâ€™re not having a conversation. Youâ€™re configuring a sampling algorithm.",
  "Let me show you each knob.",
  "Parameter 1: Max Tokens (The Length Controller)",
  "What it does:Â Hard limit on how many tokens (â‰ˆwords) the model can generate.",
  "Critical distinction:",
  "Context window = Total input + output capacity",
  "Max tokens = Output-only limit",
  "Why it matters:",
  "Cost control:",
  "GPT-4 Turbo:\n500 tokens = $0.015\n4,000 tokens = $0.12\nâ†’ 8Ã— cost difference",
  "Speed:",
  "500 tokens: 10-25 seconds\n2,000 tokens: 40-100 seconds",
  "Practical values:",
  "Chat: 300â€“500 tokens",
  "Code: 1,000â€“2,000",
  "Articles: 2,000â€“4,000",
  "JSON: 50â€“200",
  "Pro tip:Â Always set this. Default unlimited wastes money.",
  "Parameter 2: Temperature (The Creativity Dial)",
  "What it does:Â Scales logits before sampling. Controls randomness.",
  "The effect:",
  "Temperature = 0.1Â (Deterministic)",
  "\"good\": 85% â†’ Always picks this\n\"great\": 10%\n\"excellent\": 4%",
  "Temperature = 1.5Â (Creative)",
  "\"good\": 35%\n\"great\": 30% â†’ Much more variety\n\"excellent\": 20%\n\"bad\": 15%",
  "When to use what:",
  "0.0â€“0.3Â (Factual tasks)",
  "âœ… Code generation",
  "âœ… Data extraction",
  "âœ… Translation",
  "âœ… Classification",
  "0.7â€“1.0Â (Balanced)",
  "âœ… General conversation",
  "âœ… Explanations",
  "âœ… Default for most tasks",
  "1.2â€“2.0Â (Creative)",
  "âœ… Creative writing",
  "âœ… Brainstorming",
  "âœ… Story generation",
  "Danger zone:Â Temperature > 2.0 often produces nonsense.",
  "Sweet spot:Â 0.7 for most tasks.",
  "Parameter 3: Top-P / Nucleus Sampling (The Quality Filter)",
  "What it does:Â Sample only from tokens whose cumulative probability â‰¥ P. Cuts off the â€œlong tail.â€",
  "How it works:",
  "Without Top-P:",
  "Every token (including absurd ones) is a candidate",
  "With Top-P = 0.9:",
  "Cumulative probability:\n\"the\": 25% (total: 25%)\n\"a\": 20% (total: 45%)\n\"an\": 15% (total: 60%)\n\"this\": 10% (total: 70%)\n\"that\": 8% (total: 78%)\n\"those\": 7% (total: 85%)\n\"these\": 5% (total: 90%) â† STOP",
  "Only sample from these top tokens\nIgnore the nonsensical tail",
  "Why this matters:Â Prevents neural text degeneration (repetition, rambling).",
  "Practical values:",
  "1.0Â = No filtering (risky)",
  "0.95Â = Light filtering (creative)",
  "0.9Â =Â Recommended default",
  "0.5Â = Heavy filtering (conservative)",
  "Key insight:Â Anthropic advises tuning EITHER temperature OR Top-P, not both (they compound).",
  "Recommendation:Â Fix Top-P at 0.9, vary temperature.",
  "Parameter 4: Top-K (The Candidate Limiter)",
  "What it does:Â Only consider the K highest-probability tokens at each step.",
  "Difference from Top-P:",
  "Top-K = Fixed number of tokens",
  "Top-P = Variable number based on probability mass",
  "Top-K issues:",
  "Sometimes K is too many (when answer is obvious)",
  "Sometimes K is too few (when multiple good options)",
  "Top-P advantage:Â Adaptive to the situation.",
  "Modern recommendation:Â Use Top-P instead of Top-K.",
  "Most APIs (OpenAI, Anthropic) default to Top-P and ignore Top-K.",
  "Parameter 5: Frequency Penalty (The Repetition Killer)",
  "What it does:Â Reduces probability proportional to how often tokens appeared.",
  "Formula:",
  "modified_logit = original_logit - (frequency_penalty Ã— token_count)",
  "The problem it solves:",
  "Without frequency penalty:",
  "\"The best product is the best choice. \nThe best quality ensures the best results...\"",
  "With frequency penalty = 0.8:",
  "\"The best product offers superior quality. \nIts innovative design ensures excellent results...\"",
  "When to use:",
  "Long-form generation (articles, stories)",
  "Creative writing",
  "List generation",
  "Any time you see repetition",
  "When NOT to use:",
  "Technical writing (terms should repeat)",
  "Code generation",
  "Structured output (JSON/XML)",
  "Practical values:",
  "0.0Â = Default (no penalty)",
  "0.3â€“0.7Â = Mild variety",
  "0.8â€“1.5Â = Strong variety",
  "Recommended default: 0.0Â (increase only if needed)",
  "Parameter 6: Presence Penalty (The Novelty Driver)",
  "What it does:Â Penalizes tokens that appeared AT LEAST ONCE (binary, not proportional).",
  "Key difference from frequency:",
  "Frequency = Penalizes based on count",
  "Presence = Binary (used = penalized equally)",
  "Effect:",
  "With presence penalty = 1.0:",
  "Already used: \"cat\", \"sat\", \"mat\"\nAll get -1.0 penalty\nModel heavily favors NEW words",
  "Frequency vs Presence:",
  "Frequency penalty = 0.8:",
  "\"This durable phone features excellent battery. \nThe display provides clarity. Camera captures photos.\"\nâ†’ Varies WORDS, stays on TOPIC",
  "Presence penalty = 0.8:",
  "\"This durable phone features battery life. \nThe company makes laptops. Headquarters in California...\"\nâ†’ Introduces NEW TOPICS (drifts away)",
  "When to use:",
  "Brainstorming",
  "Exploratory writing",
  "Avoiding â€œgetting stuckâ€",
  "When NOT to use:",
  "Focused explanations",
  "Consistency matters",
  "Technical content",
  "Practical values:",
  "0.0Â = Default",
  "0.3â€“0.7Â = Mild novelty",
  "0.8â€“1.5Â = Strong novelty",
  "Parameter 7: Stop Sequences (The Emergency Brake)",
  "What it does:Â Forces model to halt when specific strings appear. Stop text is NOT included in output.",
  "Example:",
  "prompt = \"List 3 benefits:\\n1.\"\nstop_sequences = [\"\\n4.\", \"\\n\\n\"]",
  "Output:\n\"1. Improves health\n2. Boosts energy\n3. Increases focus\"\n[Stops at \"\\n4.\"]",
  "Why this is critical:",
  "JSON generation:",
  "stop_sequences = [\"}\"]\nâ†’ Stops exactly after closing brace",
  "Section boundaries:",
  "stop_sequences = [\"###\", \"\\n\\nReviews:\"]\nâ†’ Prevents unwanted sections",
  "Design tips:",
  "Use unambiguous delimiters:Â \"<|END|>\",Â \"\\n---\\n\"",
  "Multiple sequences for robustness",
  "Pair with max_tokens (belt-and-suspenders)",
  "Common stop sequences:",
  "JSON:Â \"}\",Â \"]\"",
  "Code:Â \"```\"",
  "Lists:Â \"\\n\\n\"",
  "Sections:Â \"###\",Â \"---\"",
  "Chat:Â \"User:\",Â \"Human:\"",
  "How Parameters Interact (The Critical Part)",
  "Most guides miss this: Parameters compound.",
  "Interaction 1: Temperature Ã— Top-P",
  "Temperature = 1.5 (flattens distribution)\nTop-P = 0.95 (keeps 95%)",
  "Effect: Controlled creativity\nTemperature spreads probability â†’ Top-P crops tail",
  "Rule:Â High temperature + Low Top-P = Controlled creativity",
  "Interaction 2: Penalties Ã— Temperature",
  "Temperature = 0.7\nFrequency penalty = 1.0",
  "Effect:\n1. Temperature sets initial distribution\n2. Penalties modify logits\n3. Distribution shifts away from used tokens\nâ†’ Increasingly novel vocabulary",
  "Danger:Â High temperature + High penalties = drift into gibberish",
  "Interaction 3: Both Penalties Together",
  "Frequency penalty = 0.5\nPresence penalty = 0.3\nâ†’ Variety without chaos",
  "VS\n\nFrequency = 1.5\nPresence = 1.5\nâ†’ Often incoherent",
  "Rule:Â If using both, keep combined total < 2.0",
  "Real Scenarios: What Settings to Use When",
  "Customer Support Chatbot",
  "Requirements:Â Accurate, consistent, fast",
  "{\n    \"max_tokens\": 300,\n    \"temperature\": 0.3,\n    \"top_p\": 0.9,\n    \"frequency_penalty\": 0.2,\n    \"presence_penalty\": 0.0,\n    \"stop\": [\"\\n\\nUser:\"]\n}",
  "Why:Â Low temp = factual, light frequency = natural language",
  "Creative Story Writing",
  "Requirements:Â Imaginative, varied, engaging",
  "{\n    \"max_tokens\": 3000,\n    \"temperature\": 1.2,\n    \"top_p\": 0.95,\n    \"frequency_penalty\": 0.7,\n    \"presence_penalty\": 0.4,\n    \"stop\": [\"### THE END\"]\n}",
  "Why:Â High temp = creative, penalties = variety",
  "Code Generation",
  "Requirements:Â Correct syntax, complete functions",
  "{\n    \"max_tokens\": 1500,\n    \"temperature\": 0.2,\n    \"top_p\": 0.9,\n    \"frequency_penalty\": 0.0,\n    \"presence_penalty\": 0.0,\n    \"stop\": [\"```\", \"\\n\\n#\"]\n}",
  "Why:Â Very low temp = correct, no penalties = terms repeat properly",
  "Data Extraction (JSON)",
  "Requirements:Â Strict format, deterministic",
  "{\n    \"max_tokens\": 200,\n    \"temperature\": 0.0,\n    \"top_p\": 1.0,\n    \"frequency_penalty\": 0.0,\n    \"presence_penalty\": 0.0,\n    \"stop\": [\"}\"]\n}",
  "Why:Â Temp 0 = identical every time, stop at closing brace",
  "The Cheat Sheet Youâ€™ll Actually Use",
  "Quick Decision Guide",
  "Task Type:",
  "Factual/TechnicalÂ â†’ temp: 0.0â€“0.3, penalties: 0.0",
  "General ChatÂ â†’ temp: 0.7â€“0.9, freq: 0.2â€“0.4",
  "CreativeÂ â†’ temp: 1.2â€“1.5, freq: 0.6â€“0.8, presence: 0.4â€“1.0",
  "Output Length:",
  "ShortÂ (chat) â†’ 200â€“500",
  "MediumÂ (descriptions) â†’ 500â€“1,000",
  "LongÂ (articles) â†’ 1,000â€“4,000",
  "Format Critical?",
  "YesÂ (JSON) â†’ temp: 0.0, stop: format delimiters",
  "NoÂ (natural text) â†’ stop: section markers",
  "Repetition Problem?",
  "YesÂ â†’ frequency_penalty: 0.5â€“0.8",
  "NoÂ â†’ frequency_penalty: 0.0",
  "The â€œStart Hereâ€ Defaults",
  "{\n    \"max_tokens\": 500,\n    \"temperature\": 0.7,\n    \"top_p\": 0.9,\n    \"frequency_penalty\": 0.0,\n    \"presence_penalty\": 0.0,\n    \"stop\": []\n}",
  "Then adjust ONE parameter at a time.",
  "The Golden Rules",
  "Always set max_tokensÂ (control cost & length)",
  "Temperature is your primary dialÂ (0â€“0.3 factual, 0.7â€“1.0 balanced, 1.2+ creative)",
  "Keep Top-P at 0.9Â unless specific reason",
  "Start penalties at 0.0Â (increase only when seeing issues)",
  "Define stop sequencesÂ for structured output",
  "Tune ONE parameter at a timeÂ (test, measure, iterate)",
  "Different tasks need different configsÂ (code â‰  creative â‰  chat)",
  "Common Mistakes to Avoid",
  "Mistake 1:Â Setting everything high",
  "# DON'T DO THIS\n{\"temperature\": 1.8, \"top_p\": 1.0, \n \"frequency_penalty\": 1.5, \"presence_penalty\": 1.5}\nâ†’ Incoherent nonsense",
  "Mistake 2:Â Ignoring max_tokens",
  "# DON'T DO THIS\n{\"max_tokens\": None}\nâ†’ Unpredictable costs, slow responses",
  "Mistake 3:Â Penalties on technical content",
  "# DON'T DO THIS for code\n{\"frequency_penalty\": 0.8}\nâ†’ Variable names change mid-code",
  "Mistake 4:Â No stop sequences",
  "# DON'T DO THIS for JSON\n{\"stop\": []}\nâ†’ Continues past closing brace",
  "The Bottom Line",
  "Youâ€™re not just â€œusing AI.â€ Youâ€™re programming a probability sampling system.",
  "These 7 parameters are your programming interface:",
  "Max TokensÂ = Length limit",
  "TemperatureÂ = Creativity dial",
  "Top-PÂ = Quality filter",
  "Top-KÂ = Candidate limiter (use Top-P instead)",
  "Frequency PenaltyÂ = Repetition killer",
  "Presence PenaltyÂ = Novelty driver",
  "Stop SequencesÂ = Emergency brake",
  "Master these, and youâ€™ll get 10Ã— better outputs than users who donâ€™t.",
  "The meta-lesson:Â Same prompt + different parameters = completely different results.",
  "Now you know the control panel. Time to experiment.",
  "Your Next Steps",
  "Beginner?",
  "Start with the defaults above",
  "Experiment with temperature first",
  "Practice on different tasks",
  "Intermediate?",
  "A/B test on your actual use cases",
  "Measure impact of each parameter",
  "Build task-specific configs",
  "Advanced?",
  "Dynamic parameter selection",
  "Track metrics and iterate",
  "Share what you learn",
  "Quick Reference Card",
  "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nPARAMETER QUICK REFERENCE\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
  "MAX TOKENS\nâ”œâ”€ Chat: 300-500300-500\nâ”œâ”€ Code: 1000-2000\nâ””â”€ Articles: 2000-4000\nTEMPERATURE\nâ”œâ”€ Factual: 0.0-0.3\nâ”œâ”€ Balanced: 0.7-0.9\nâ””â”€ Creative: 1.2-1.5\nTOP-P (Default: 0.9)\nâ”œâ”€ Conservative: 0.5-0.7\nâ””â”€ Exploratory: 0.95-1.0\nFREQUENCY PENALTY (Default: 0.0)\nâ””â”€ If repetition: 0.3-0.8\nPRESENCE PENALTY (Default: 0.0)\nâ””â”€ If stuck on topic: 0.3-1.0\nSTOP SEQUENCES\nâ”œâ”€ JSON: \"}\", \"]\"\nâ”œâ”€ Code: \"```\"\nâ””â”€ Custom: \"<|END|>\", \"###\"\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\nGOLDEN RULES\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n1. Always set max_tokens\n2. Tune temp OR top_p (not both)\n3. Start penalties at 0.0\n4. One parameter change at a time\n5. Test, measure, iterate\nâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”",
  "Most AI tutorials teach you prompts. This taught you the control panel.",
  "Prompts are instructions. Parameters are the execution environment.",
  "Same prompt + different parameters = completely different results.",
  "Now go configure some probability distributions. ğŸ›ï¸"
]