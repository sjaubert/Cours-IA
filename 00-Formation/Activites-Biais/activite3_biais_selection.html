<!DOCTYPE html>
<html lang="fr">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Activit√© 3 - Le Biais de S√©lection des Donn√©es</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            background: linear-gradient(135deg, #fa709a 0%, #fee140 100%);
            min-height: 100vh;
            padding: 2rem;
            color: #333;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            background: rgba(255, 255, 255, 0.95);
            border-radius: 20px;
            padding: 3rem;
            box-shadow: 0 20px 60px rgba(0, 0, 0, 0.3);
        }

        h1 {
            color: #fa709a;
            font-size: 2.5rem;
            margin-bottom: 1rem;
            text-align: center;
        }

        .subtitle {
            text-align: center;
            color: #666;
            font-size: 1.1rem;
            margin-bottom: 2rem;
            font-style: italic;
        }

        .duration {
            text-align: center;
            background: #17a2b8;
            color: white;
            padding: 1rem;
            border-radius: 10px;
            margin: 1rem 0;
            font-size: 1.1rem;
            font-weight: bold;
        }

        .section {
            margin: 2rem 0;
            padding: 1.5rem;
            background: #f8f9fa;
            border-radius: 10px;
            border-left: 5px solid #fa709a;
        }

        .section h2 {
            color: #fa709a;
            margin-bottom: 1rem;
            font-size: 1.8rem;
        }

        .section h3 {
            color: #ff8a65;
            margin: 1.5rem 0 1rem 0;
            font-size: 1.4rem;
        }

        .objetifs {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 2rem;
            border-radius: 15px;
            margin: 2rem 0;
        }

        .objetifs h2 {
            color: white;
            margin-bottom: 1rem;
        }

        .objetifs ul {
            list-style: none;
            padding-left: 0;
        }

        .objetifs li {
            padding: 0.5rem 0;
            padding-left: 1.5rem;
            position: relative;
        }

        .objetifs li::before {
            content: "‚úì";
            position: absolute;
            left: 0;
            font-weight: bold;
        }

        .case-study {
            background: white;
            padding: 2rem;
            margin: 1.5rem 0;
            border-radius: 10px;
            box-shadow: 0 8px 16px rgba(0, 0, 0, 0.1);
            border-top: 5px solid #fa709a;
        }

        .case-header {
            display: flex;
            align-items: center;
            margin-bottom: 1.5rem;
            padding-bottom: 1rem;
            border-bottom: 2px solid #f8f9fa;
        }

        .case-icon {
            width: 60px;
            height: 60px;
            background: linear-gradient(135deg, #fa709a 0%, #fee140 100%);
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 1.8rem;
            margin-right: 1.5rem;
            flex-shrink: 0;
        }

        .case-title {
            flex-grow: 1;
        }

        .case-title h3 {
            color: #fa709a;
            margin: 0;
            font-size: 1.5rem;
        }

        .case-title p {
            color: #666;
            margin: 0.3rem 0 0 0;
            font-style: italic;
        }

        .stat-box {
            background: linear-gradient(135deg, #f093fb 0%, #f5576c 100%);
            color: white;
            padding: 1.5rem;
            border-radius: 10px;
            margin: 1rem 0;
            text-align: center;
        }

        .stat-number {
            font-size: 3rem;
            font-weight: bold;
            margin-bottom: 0.5rem;
        }

        .stat-label {
            font-size: 1.1rem;
        }

        .impact-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 1rem;
            margin: 1.5rem 0;
        }

        .impact-card {
            background: white;
            padding: 1.5rem;
            border-radius: 10px;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
            border-left: 5px solid #fa709a;
        }

        .impact-card h4 {
            color: #fa709a;
            margin-bottom: 1rem;
        }

        .analysis-section {
            background: #fff3cd;
            padding: 1.5rem;
            border-radius: 10px;
            margin: 1.5rem 0;
            border-left: 5px solid #ffc107;
        }

        .analysis-section h4 {
            color: #856404;
            margin-bottom: 1rem;
        }

        textarea {
            width: 100%;
            min-height: 100px;
            padding: 1rem;
            border: 2px solid #e9ecef;
            border-radius: 8px;
            font-family: inherit;
            font-size: 1rem;
            resize: vertical;
            transition: border-color 0.3s ease;
        }

        textarea:focus {
            outline: none;
            border-color: #fa709a;
        }

        .warning-box {
            background: #f8d7da;
            border-left: 5px solid #dc3545;
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-radius: 10px;
            color: #721c24;
        }

        .info-box {
            background: #d1ecf1;
            border-left: 5px solid #17a2b8;
            padding: 1.5rem;
            margin: 1.5rem 0;
            border-radius: 10px;
            color: #0c5460;
        }

        .timeline {
            position: relative;
            padding-left: 2rem;
            margin: 2rem 0;
        }

        .timeline::before {
            content: '';
            position: absolute;
            left: 0;
            top: 0;
            bottom: 0;
            width: 3px;
            background: linear-gradient(135deg, #fa709a 0%, #fee140 100%);
        }

        .timeline-item {
            position: relative;
            margin-bottom: 2rem;
            padding-left: 2rem;
        }

        .timeline-item::before {
            content: '';
            position: absolute;
            left: -2rem;
            top: 0;
            width: 15px;
            height: 15px;
            background: #fa709a;
            border-radius: 50%;
            transform: translateX(-6px);
        }

        .timeline-item h4 {
            color: #fa709a;
            margin-bottom: 0.5rem;
        }

        table {
            width: 100%;
            border-collapse: collapse;
            margin: 1rem 0;
            background: white;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
        }

        th {
            background: #fa709a;
            color: white;
            padding: 1rem;
            text-align: left;
        }

        td {
            padding: 1rem;
            border-bottom: 1px solid #e9ecef;
        }

        tr:hover {
            background: #f8f9fa;
        }

        .btn {
            background: linear-gradient(135deg, #fa709a 0%, #fee140 100%);
            color: white;
            padding: 1rem 2rem;
            border: none;
            border-radius: 25px;
            font-size: 1.1rem;
            cursor: pointer;
            transition: transform 0.3s ease, box-shadow 0.3s ease;
            margin: 1rem 0;
        }

        .btn:hover {
            transform: translateY(-3px);
            box-shadow: 0 10px 20px rgba(250, 112, 154, 0.4);
        }

        .question-box {
            background: white;
            padding: 1.5rem;
            border-radius: 10px;
            margin: 1rem 0;
            border-left: 5px solid #28a745;
        }

        .uimm-header {
            background: white;
            padding: 1.5rem 2rem;
            margin-bottom: 2rem;
            border-radius: 15px 15px 0 0;
            display: flex;
            align-items: center;
            justify-content: space-between;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
        }

        .uimm-logo {
            max-height: 60px;
            height: auto;
        }

        .uimm-title {
            color: #2c3e50;
            font-size: 1.1rem;
            font-weight: 600;
        }

        .uimm-footer {
            background: #2c3e50;
            color: white;
            padding: 1.5rem;
            text-align: center;
            margin-top: 3rem;
            border-radius: 0 0 15px 15px;
            font-size: 0.95rem;
        }

        @media (max-width: 768px) {
            .container {
                padding: 1.5rem;
            }

            h1 {
                font-size: 2rem;
            }

            .impact-grid {
                grid-template-columns: 1fr;
            }

            .uimm-header {
                flex-direction: column;
                text-align: center;
                gap: 1rem;
            }
        }
    </style>
</head>

<body>
    <div class="container">
        <div class="uimm-header">
            <img src="logo_uimm.jpg" alt="UIMM Logo" class="uimm-logo">
            <div class="uimm-title">P√¥le Formation UIMM CVDL</div>
        </div>

        <h1>Activit√© 3 : Le Biais de S√©lection des Donn√©es</h1>
        <p class="subtitle">Quand les donn√©es d'entra√Ænement cr√©ent des discriminations syst√©miques</p>

        <div class="duration">Dur√©e : 60 minutes</div>

        <div class="objetifs">
            <h2>Objectifs</h2>
            <ul>
                <li>Comprendre comment les donn√©es biais√©es cr√©ent des IA discriminantes</li>
                <li>Analyser des cas r√©els de biais de s√©lection</li>
                <li>Identifier les sources de biais dans les donn√©es</li>
                <li>Proposer des strat√©gies pour des donn√©es plus √©quitables</li>
            </ul>
        </div>

        <div class="section">
            <h2>Introduction : Le Principe "Garbage In, Garbage Out"</h2>
            <p>Le biais de s√©lection survient lorsque les donn√©es utilis√©es pour entra√Æner un syst√®me d'IA ne sont pas
                repr√©sentatives de la population ou de la r√©alit√© qu'il est cens√© mod√©liser. C'est l'un des biais les
                plus courants et les plus dangereux car il est souvent invisible jusqu'√† ce que le syst√®me soit d√©ploy√©.
            </p>

            <div class="info-box">
                <h4>Principe fondamental</h4>
                <p>Une IA n'apprend que ce qui est pr√©sent dans ses donn√©es d'entra√Ænement. Si les donn√©es sont
                    biais√©es, l'IA sera biais√©e. Si certains groupes sont sous-repr√©sent√©s ou mal repr√©sent√©s, l'IA ne
                    saura pas bien les traiter. C'est math√©matiquement in√©vitable.</p>
            </div>
        </div>

        <div class="section">
            <h2>Cas d'√âtude 1 : Amazon et le Recrutement Automatis√©</h2>

            <div class="case-study">
                <div class="case-header">
                    <div class="case-icon">üè¢</div>
                    <div class="case-title">
                        <h3>Amazon HR Tool (2014-2018)</h3>
                        <p>Discrimination syst√©matique des candidatures f√©minines</p>
                    </div>
                </div>

                <h3>Contexte</h3>
                <p>En 2014, Amazon a d√©velopp√© un outil d'IA pour automatiser le tri des CV et identifier les meilleurs
                    candidats pour des postes techniques. L'objectif √©tait de gagner du temps et d'am√©liorer
                    l'objectivit√© du recrutement.</p>

                <h3>Les Donn√©es d'Entra√Ænement</h3>
                <div class="timeline">
                    <div class="timeline-item">
                        <h4>Source des donn√©es</h4>
                        <p>10 ans de CV soumis √† Amazon (2004-2014)</p>
                    </div>
                    <div class="timeline-item">
                        <h4>Composition</h4>
                        <p>Majorit√© √©crasante de CV masculins pour les postes techniques</p>
                    </div>
                    <div class="timeline-item">
                        <h4>Historique de recrutement</h4>
                        <p>Principalement des hommes embauch√©s dans le pass√© pour ces postes</p>
                    </div>
                </div>

                <h3>Le Biais D√©couvert</h3>
                <div class="warning-box">
                    <p><strong>Probl√®me identifi√© :</strong> L'IA p√©nalisait syst√©matiquement :</p>
                    <ul style="margin-left: 2rem; margin-top: 1rem; line-height: 2;">
                        <li>Les CV contenant le mot "femme" (ex: "capitaine de l'√©quipe de football f√©minine")</li>
                        <li>Les dipl√¥m√©es de deux universit√©s exclusivement f√©minines</li>
                        <li>Subtilement, d'autres indicateurs corr√©l√©s au genre</li>
                    </ul>
                </div>

                <h3>Pourquoi est-ce arriv√© ?</h3>
                <div class="impact-grid">
                    <div class="impact-card">
                        <h4>Donn√©es historiques biais√©es</h4>
                        <p>Le secteur tech √©tait (et reste) majoritairement masculin. Les donn√©es refl√©taient cette
                            r√©alit√© historique discriminatoire.</p>
                    </div>
                    <div class="impact-card">
                        <h4>Apprentissage de corr√©lations</h4>
                        <p>L'IA a appris que "√™tre un homme" √©tait corr√©l√© avec "√™tre recrut√©". Elle a transform√© cette
                            corr√©lation en crit√®re de s√©lection.</p>
                    </div>
                    <div class="impact-card">
                        <h4>Amplification du biais</h4>
                        <p>Au lieu de corriger le biais historique, l'IA l'a automatis√© et amplifi√© en le rendant
                            syst√©matique.</p>
                    </div>
                </div>

                <h3>Cons√©quence</h3>
                <div class="stat-box">
                    <div class="stat-number">2018</div>
                    <div class="stat-label">Projet abandonn√© apr√®s 4 ans de tentatives de correction</div>
                </div>
                <p style="text-align: center; margin-top: 1rem;">Amazon a finalement abandonn√© le projet, reconnaissant
                    l'impossibilit√© de garantir l'absence de biais avec ces donn√©es.</p>

                <div class="analysis-section">
                    <h4>Votre analyse - Groupe 1</h4>
                    <p><strong>Question :</strong> Identifiez au moins 3 d√©cisions diff√©rentes qui auraient pu √™tre
                        prises en amont pour √©viter ce probl√®me.</p>
                    <textarea></textarea>
                </div>
            </div>
        </div>

        <div class="section">
            <h2>Cas d'√âtude 2 : COMPAS et la Justice Pr√©dictive</h2>

            <div class="case-study">
                <div class="case-header">
                    <div class="case-icon">‚öñÔ∏è</div>
                    <div class="case-title">
                        <h3>COMPAS (Correctional Offender Management Profiling)</h3>
                        <p>Biais racial dans l'√©valuation du risque de r√©cidive</p>
                    </div>
                </div>

                <h3>Contexte</h3>
                <p>COMPAS est un algorithme utilis√© par les tribunaux am√©ricains pour √©valuer le risque qu'un pr√©venu
                    r√©cidive. Il influence les d√©cisions de lib√©ration conditionnelle et de d√©termination des peines.
                </p>

                <h3>Le Biais D√©couvert (ProPublica, 2016)</h3>

                <table>
                    <thead>
                        <tr>
                            <th>Groupe</th>
                            <th>Taux de faux positifs</th>
                            <th>Taux de faux n√©gatifs</th>
                            <th>Impact</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Pr√©venus noirs</strong></td>
                            <td>45%</td>
                            <td>23%</td>
                            <td>Risque sur√©valu√© : personnes class√©es √† haut risque alors qu'elles ne r√©cidivaient pas
                            </td>
                        </tr>
                        <tr>
                            <td><strong>Pr√©venus blancs</strong></td>
                            <td>23%</td>
                            <td>48%</td>
                            <td>Risque sous-√©valu√© : personnes class√©es √† faible risque mais qui r√©cidivaient</td>
                        </tr>
                    </tbody>
                </table>

                <div class="warning-box">
                    <h4>Cons√©quence concr√®te</h4>
                    <p>√Ä niveau de risque r√©el √©quivalent, une personne noire avait 2 fois plus de chances d'√™tre
                        class√©e "haut risque" qu'une personne blanche, impactant directement la dur√©e d'incarc√©ration.
                    </p>
                </div>

                <h3>Sources du Biais</h3>
                <div class="impact-grid">
                    <div class="impact-card">
                        <h4>Donn√©es historiques</h4>
                        <p>Entra√Æn√© sur des donn√©es d'arrestations et de condamnations pass√©es, d√©j√† biais√©es par le
                            profilage racial dans les forces de l'ordre.</p>
                    </div>
                    <div class="impact-card">
                        <h4>Variables proxy</h4>
                        <p>L'algorithme utilisait des variables comme le code postal, le niveau d'√©ducation, qui sont
                            des "proxies" (substituts) pour la race.</p>
                    </div>
                    <div class="impact-card">
                        <h4>Boucle de r√©troaction</h4>
                        <p>Plus de surveillance dans certains quartiers = plus d'arrestations = donn√©es encore plus
                            biais√©es.</p>
                    </div>
                </div>

                <div class="analysis-section">
                    <h4>Votre analyse - Groupe 2</h4>
                    <p><strong>Question :</strong> Pourquoi est-il particuli√®rement probl√©matique d'utiliser l'IA dans
                        le syst√®me judiciaire ? Quels principes fondamentaux sont en jeu ?</p>
                    <textarea></textarea>
                </div>
            </div>
        </div>

        <div class="section">
            <h2>Cas d'√âtude 3 : Reconnaissance Faciale et Biais Ethnique</h2>

            <div class="case-study">
                <div class="case-header">
                    <div class="case-icon">üì∏</div>
                    <div class="case-title">
                        <h3>Syst√®mes de Reconnaissance Faciale</h3>
                        <p>Taux d'erreur variables selon l'ethnie et le genre</p>
                    </div>
                </div>

                <h3>R√©sultats de l'√âtude MIT (2018)</h3>

                <table>
                    <thead>
                        <tr>
                            <th>Groupe d√©mographique</th>
                            <th>Taux d'erreur moyen</th>
                            <th>Diff√©rence</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Hommes √† peau claire</strong></td>
                            <td>0.8%</td>
                            <td>R√©f√©rence</td>
                        </tr>
                        <tr>
                            <td><strong>Femmes √† peau claire</strong></td>
                            <td>7.1%</td>
                            <td>√ó9</td>
                        </tr>
                        <tr>
                            <td><strong>Hommes √† peau fonc√©e</strong></td>
                            <td>12.0%</td>
                            <td>√ó15</td>
                        </tr>
                        <tr>
                            <td><strong>Femmes √† peau fonc√©e</strong></td>
                            <td>34.7%</td>
                            <td>√ó43</td>
                        </tr>
                    </tbody>
                </table>

                <div class="stat-box">
                    <div class="stat-number">√ó43</div>
                    <div class="stat-label">Une femme noire a 43 fois plus de risques d'√™tre mal identifi√©e qu'un homme
                        blanc</div>
                </div>

                <h3>Origine du Biais</h3>
                <div class="info-box">
                    <h4>Datasets d√©s√©quilibr√©s</h4>
                    <p>Les datasets d'entra√Ænement (comme ImageNet, LFW) contenaient majoritairement des visages de
                        personnes blanches et masculines, souvent issus de contextes occidentaux. Les femmes et les
                        personnes de couleur √©taient largement sous-repr√©sent√©es.</p>
                </div>

                <h3>Impacts R√©els</h3>
                <ul style="line-height: 2; margin-left: 2rem;">
                    <li><strong>Erreurs d'identification judiciaire :</strong> Plusieurs cas d'arrestations erron√©es de
                        personnes noires</li>
                    <li><strong>Acc√®s refus√© :</strong> Syst√®mes de s√©curit√© qui ne reconnaissent pas certaines
                        personnes</li>
                    <li><strong>Surveillance disproportionn√©e :</strong> Plus de "faux positifs" dans certaines
                        communaut√©s</li>
                </ul>

                <div class="analysis-section">
                    <h4>Votre analyse - Groupe 3</h4>
                    <p><strong>Question :</strong> Comment pourrait-on constituer un dataset d'entra√Ænement r√©ellement
                        repr√©sentatif pour la reconnaissance faciale ? Quels d√©fis anticipez-vous ?</p>
                    <textarea></textarea>
                </div>
            </div>
        </div>

        <div class="section">
            <h2>Synth√®se : Les Sources de Biais de S√©lection</h2>

            <table>
                <thead>
                    <tr>
                        <th>Type de biais</th>
                        <th>Description</th>
                        <th>Exemple</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Sous-repr√©sentation</strong></td>
                        <td>Certains groupes sont peu pr√©sents dans les donn√©es</td>
                        <td>Peu de femmes dans les donn√©es de recrutement tech</td>
                    </tr>
                    <tr>
                        <td><strong>Donn√©es historiques biais√©es</strong></td>
                        <td>Les donn√©es refl√®tent des discriminations pass√©es</td>
                        <td>Historique d'arrestations avec profilage racial</td>
                    </tr>
                    <tr>
                        <td><strong>Biais de collecte</strong></td>
                        <td>La m√©thode de collecte favorise certains groupes</td>
                        <td>Photos collect√©es principalement dans des pays occidentaux</td>
                    </tr>
                    <tr>
                        <td><strong>Variables proxy</strong></td>
                        <td>Utilisation de variables corr√©l√©es √† des attributs prot√©g√©s</td>
                        <td>Code postal comme proxy de l'origine ethnique</td>
                    </tr>
                    <tr>
                        <td><strong>Biais de survie</strong></td>
                        <td>Seuls les "succ√®s" sont dans les donn√©es</td>
                        <td>Uniquement les CV de personnes embauch√©es par le pass√©</td>
                    </tr>
                    <tr>
                        <td><strong>Biais temporel</strong></td>
                        <td>Les donn√©es sont obsol√®tes</td>
                        <td>Mod√®les m√©dicaux bas√©s sur des √©tudes d'il y a 20 ans</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <div class="section">
            <h2>Exercice de Groupe : Audit de Dataset</h2>

            <div class="question-box">
                <h3>Sc√©nario</h3>
                <p>Votre entreprise souhaite d√©velopper un outil d'IA pour l'un des cas suivants (choisissez-en un) :
                </p>
                <ul style="margin-left: 2rem; line-height: 2;">
                    <li><strong>A.</strong> Assistant IA pour le service client</li>
                    <li><strong>B.</strong> Syst√®me de recommandation de formations pour les employ√©s</li>
                    <li><strong>C.</strong> Outil d'analyse de CV pour le recrutement</li>
                    <li><strong>D.</strong> Chatbot de conseil en orientation de carri√®re</li>
                </ul>
            </div>

            <div class="question-box">
                <h3>Cas choisi :</h3>
                <textarea style="min-height: 50px;"></textarea>
            </div>

            <div class="question-box">
                <h3>Question 1 : Identification des risques</h3>
                <p>Quels types de biais de s√©lection pourraient affecter les donn√©es d'entra√Ænement de ce syst√®me ?</p>
                <textarea></textarea>
            </div>

            <div class="question-box">
                <h3>Question 2 : Sources de donn√©es</h3>
                <p>D'o√π viendraient les donn√©es d'entra√Ænement ? Quels groupes risquent d'√™tre sous-repr√©sent√©s ou mal
                    repr√©sent√©s ?</p>
                <textarea></textarea>
            </div>

            <div class="question-box">
                <h3>Question 3 : Impact potentiel</h3>
                <p>Si ces biais ne sont pas corrig√©s, quel impact concret pourrait-il y avoir sur les utilisateurs ?</p>
                <textarea></textarea>
            </div>

            <div class="question-box">
                <h3>Question 4 : Strat√©gies de mitigation</h3>
                <p>Proposez au moins 3 mesures concr√®tes pour r√©duire les biais de s√©lection dans votre cas.</p>
                <textarea></textarea>
            </div>
        </div>

        <div class="section">
            <h2>Bonnes Pratiques pour des Donn√©es √âquitables</h2>

            <div class="impact-grid">
                <div class="impact-card">
                    <h4>1. Audit de repr√©sentativit√©</h4>
                    <p>Analyser la composition d√©mographique des donn√©es et identifier les d√©s√©quilibres.</p>
                </div>
                <div class="impact-card">
                    <h4>2. Diversification active</h4>
                    <p>Collecter intentionnellement des donn√©es de groupes sous-repr√©sent√©s.</p>
                </div>
                <div class="impact-card">
                    <h4>3. Donn√©es synth√©tiques</h4>
                    <p>G√©n√©rer des donn√©es artificielles pour √©quilibrer les groupes minoritaires.</p>
                </div>
                <div class="impact-card">
                    <h4>4. Validation crois√©e</h4>
                    <p>Tester les performances sur diff√©rents sous-groupes d√©mographiques.</p>
                </div>
                <div class="impact-card">
                    <h4>5. Transparence</h4>
                    <p>Documenter la composition des donn√©es et les limites connues.</p>
                </div>
                <div class="impact-card">
                    <h4>6. √âquipes diverses</h4>
                    <p>Inclure des perspectives vari√©es dans la collecte et l'analyse des donn√©es.</p>
                </div>
            </div>
        </div>

        <div class="section">
            <h2>Points Cl√©s √† Retenir</h2>

            <div class="info-box">
                <ul style="list-style: none; padding-left: 0;">
                    <li style="margin: 0.5rem 0;">‚úì Les donn√©es biais√©es cr√©ent in√©vitablement des IA biais√©es</li>
                    <li style="margin: 0.5rem 0;">‚úì Le biais de s√©lection est souvent invisible jusqu'au d√©ploiement
                    </li>
                    <li style="margin: 0.5rem 0;">‚úì Les donn√©es historiques perp√©tuent les discriminations pass√©es</li>
                    <li style="margin: 0.5rem 0;">‚úì La repr√©sentativit√© doit √™tre intentionnellement recherch√©e</li>
                    <li style="margin: 0.5rem 0;">‚úì L'audit des donn√©es est une √©tape essentielle avant tout
                        d√©veloppement</li>
                    <li style="margin: 0.5rem 0;">‚úì Certains biais peuvent √™tre impossibles √† corriger compl√®tement</li>
                </ul>
            </div>
        </div>

        <div style="text-align: center; margin-top: 3rem;">
            <div style="display: inline-flex; gap: 1rem; flex-wrap: wrap; justify-content: center;">
                <button class="btn" onclick="window.location.href='activite2_premiere_impression.html'"
                    style="background: linear-gradient(135deg, #6c757d 0%, #495057 100%);">
                    ‚Üê Activit√© pr√©c√©dente
                </button>
                <button class="btn" onclick="window.location.href='index.html'"
                    style="background: linear-gradient(135deg, #6c757d 0%, #495057 100%);">
                    Retour √† l'accueil
                </button>
                <button class="btn" onclick="window.location.href='activite4_biais_confirmation.html'"
                    style="background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);">
                    Activit√© suivante ‚Üí
                </button>
            </div>
        </div>

        <div class="uimm-footer">
            P√¥le Formation UIMM CVDL - Formation Intelligence Artificielle : Les Biais
        </div>
    </div>
</body>

</html>